import os
from crawler.url_collector import collect_urls
from crawler.utils import load_articles_from_urls, save_json
from analyzer.nlp_processor import NLPProcessor
from config.constants import PROCESSED_DATA_DIR


def build_dataset(site_key="cnn", keywords=["trump"], limit=10):
    print("ğŸ” Step 1. ê¸°ì‚¬ URL ìˆ˜ì§‘")
    urls = collect_urls(site_key=site_key, keywords=keywords, limit=limit)
    print(f"ğŸ”— ìˆ˜ì§‘ëœ URL ìˆ˜: {len(urls)}")

    print("ğŸ“° Step 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘")
    articles = load_articles_from_urls(urls)
    print(f"ğŸ“„ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}")

    print("ğŸ§  Step 3. NLP ì²˜ë¦¬ ì‹œì‘")
    nlp = NLPProcessor()
    processed_articles = nlp.process_articles(articles)

    dataset = []
    for article in processed_articles:
        dataset.append({
            "url": article["url"],
            "title": article["title"],
            "summary": article["summary"],
            "cause_effects": article["cause_effects"]
        })

    print(f"ğŸ“¦ Step 4. ë°ì´í„°ì…‹ ì €ì¥ ì¤‘ ({len(dataset)}ê°œ ê¸°ì‚¬)")
    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)
    save_path = os.path.join(PROCESSED_DATA_DIR, f"{site_key}_dataset.json")
    save_json(dataset, save_path)

    print(f"âœ… ì™„ë£Œ: {save_path}")


if __name__ == "__main__":
    build_dataset()
