import requests
from bs4 import BeautifulSoup
from config import site_settings

class NewsScraper:
    def __init__(self, site="bbc", query="trump", max_pages=5):
        self.site = site
        self.settings = site_settings.get(site)
        if not self.settings:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site}")

        self.query = query
        self.max_pages = max_pages
        self.headers = {"User-Agent": "Mozilla/5.0"}
        self.excluded_keywords = {"sport", "audio"}

    def fetch_news_links(self):
        """ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (í˜ì´ì§• í¬í•¨)"""
        news_links = set()
        for page in range(1, self.max_pages + 1):
            search_url = self.settings["search_url"].format(query=self.query, page=page)
            response = requests.get(search_url, headers=self.headers)
            soup = BeautifulSoup(response.text, "html.parser")

            articles_section = soup.find_all("div", {"data-testid": self.settings["target_class"]})
            for section in articles_section:
                links = section.find_all("a", href=True)
                for a_tag in links:
                    link = a_tag["href"]
                    if link and not any(keyword in link for keyword in self.excluded_keywords):
                        full_url = link if link.startswith("http") else f"{self.settings['base_url']}{link}"
                        news_links.add(full_url)

        return list(news_links)

    def scrape_article(self, url):
        """ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§"""
        response = requests.get(url, headers=self.headers)
        if response.status_code != 200:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}")
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        title = soup.find(self.settings["title_tag"])
        title = title.text.strip() if title else "No Title"

        date_tag = soup.find(self.settings["date_tag"])
        date = date_tag["datetime"] if date_tag and date_tag.has_attr("datetime") else "No Date"

        article_tag = soup.find("article")
        content = " ".join([p.text.strip() for p in article_tag.find_all("p") if p.text.strip()]) if article_tag else "No content"

        return {"url": url, "title": title, "date": date, "content": content}

    def run(self, num_articles=5):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        news_links = self.fetch_news_links()
        print(f"ğŸ”— ì´ {len(news_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ìŒ.")

        for link in news_links[:num_articles]:
            article = self.scrape_article(link)
            if article:
                print(f"ğŸ”— URL: {article['url']}\nğŸ“° ì œëª©: {article['title']}\nğŸ“… ë‚ ì§œ: {article['date']}\nğŸ“œ ë³¸ë¬¸ (ì¼ë¶€): {article['content'][:100]}...\n")