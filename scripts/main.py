import os
from crawler.url_collector import collect_urls
from crawler.utils import load_articles_from_urls, save_json
from analyzer.nlp_processor import NLPProcessor
from config.constants import PROCESSED_DATA_DIR


def build_dataset(site_key="cnn", keywords=["trump"], limit=10):

    print("ğŸ” Step 1. ê¸°ì‚¬ URL ìˆ˜ì§‘")
    urls = collect_urls(site_key=site_key, keywords=keywords, limit=limit)

    print("ğŸ“° Step 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘")
    articles = load_articles_from_urls(urls)

    print("ğŸ§  Step 3. NLP ì²˜ë¦¬ ì‹œì‘")
    nlp = NLPProcessor()  # NLPProcessor ê°ì²´ ìƒì„±
    dataset = []

    for article in articles:
        cleaned_text = nlp.clean_text(article["text"])

        # 1. ìš”ì•½
        summary = nlp.summarize(cleaned_text)

        # 2. ì›ì¸/ê²°ê³¼ ì¶”ì¶œ
        cause_effect_pairs = nlp.extract_cause_effect(cleaned_text)

        for cause_effect in cause_effect_pairs:
            dataset.append({
                "url": article["url"],
                "title": article["title"],
                "summary": summary,
                "cause": cause_effect["cause"],
                "effect": cause_effect["effect"]
            })

    print(f"ğŸ“¦ Step 4. ë°ì´í„°ì…‹ ì €ì¥ ì¤‘ ({len(dataset)}ê°œ)")
    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)
    save_path = os.path.join(PROCESSED_DATA_DIR, f"{site_key}_dataset.json")
    save_json(dataset, save_path)

    print(f"âœ… ì™„ë£Œ: {save_path}")


if __name__ == "__main__":
    build_dataset()
